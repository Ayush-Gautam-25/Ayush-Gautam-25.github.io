---
layout: default
title: Ayush Gautam â€“ LLM Efficiency Blog
---

<style>
.navbar {
  background-color: #f2f2f2;
  padding: 10px;
  margin-bottom: 30px;
  border-radius: 8px;
}
.navbar a {
  margin-right: 20px;
  text-decoration: none;
  font-weight: bold;
}
</style>

<div class="navbar">
  <a href="/">Home</a>
  <a href="#about">About</a>
  <a href="#posts">Blog</a>
  <a href="https://github.com/Ayush-Gautam-25" target="_blank">GitHub</a>
</div>

# ğŸ‘‹ Welcome to My LLM Blog

This blog tracks my experiments in training large language models more efficiently, focusing on methods like LoRA, QLoRA, Mixture-of-Experts, data pruning, and more.

---

## ğŸ§‘â€ğŸ’» About Me {#about}

Iâ€™m Ayush Gautam, an independent AI researcher passionate about scalable training, efficient deep learning, and building open, modular LLM systems.  
This blog is a log of my journey across four levels of efficiency in LLMs:

1. **Foundational tricks** (quantization, LoRA, etc.)
2. **Architectural shifts** (Switch Transformer, MoE)
3. **Training-scale optimization** (batching, sharding, compute reuse)
4. **Infrastructure-level mastery** (parallelism, failure resilience, caching)

---

## ğŸ“ Blog Posts {#posts}

- [LoRA vs QLoRA vs Full Finetuning](./blogs/lora_vs_qlora.html)
- [Switch Transformer: MoE Efficiency](./blogs/switch_moe.html)

---

Thanks for visiting! More coming soon ğŸš€
